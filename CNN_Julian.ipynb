{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dfa08c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01046257-5167-465b-b119-42d118db081b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn\n",
    "# !pip install scikit-image\n",
    "# !pip install statsmodels\n",
    "# !pip install tensorflow\n",
    "# !pip install pydot\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d0ff50-edab-4940-ae65-de168fef9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"training.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc002ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import load_references, save_predictions\n",
    "from preprocess import *\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dbd17",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e211183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\t Dateien wurden geladen.\n"
     ]
    }
   ],
   "source": [
    "ecg_leads, ecg_labels, fs, ecg_names = load_references(folder='/shared_data/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d7172",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58542dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_leads_edited = ecg_leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0884f",
   "metadata": {},
   "source": [
    "### Noise removal (fan_multiscaled_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3f82af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = []\n",
    "for j, data in enumerate(ecg_leads_edited):\n",
    "    data_ftt, freq = ecg_furier(ecg_leads_edited[j], fs)\n",
    "    lowpassed = ecg_denoise_spectrum(data_ftt, freq, 0, 60)\n",
    "    filtered.append(ecg_invfurier(lowpassed))\n",
    "\n",
    "ecg_leads_edited = filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5707d67",
   "metadata": {},
   "source": [
    "### Z-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7ef2a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = []\n",
    "for j, data in enumerate(ecg_leads_edited):\n",
    "    filtered.append(ecg_norm(ecg_leads_edited[j]))\n",
    "\n",
    "ecg_leads_edited = filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c37446-9c40-4bb4-b388-a19e052fdf59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Length normalization (hsieh_detection_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e71ea78-7b38-42d8-8a3d-ea2b9ad833e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecg_split(data, limit, label):\n",
    "    splitted = []\n",
    "    ratio = len(data)/limit\n",
    "\n",
    "    # If data is longer than limit, split with at least 50% overlap\n",
    "    if ratio > 1:\n",
    "        for i in range(0, int(np.ceil(2*ratio))):\n",
    "            if i == int(np.ceil(2*ratio))-1:\n",
    "                string = data[len(data)-limit:len(data)]\n",
    "            else:\n",
    "                string = data[int(np.floor(i*limit/2)):int(np.floor(i*limit/2+limit))]\n",
    "            splitted.append(string)\n",
    "        splitted = [x for x in splitted if x.shape[0] >= limit] # remove string which are shorter\n",
    "        labels_multiplied = [label] * len(splitted)\n",
    "\n",
    "    # If data is shorter than limit, add from the beginning\n",
    "    elif ratio < 1:\n",
    "        if ratio <= 0.5:\n",
    "            data = np.tile(data, int(np.floor(1/ratio)))\n",
    "        diff = limit - len(data)\n",
    "        appended_data = data[0:diff]\n",
    "        data = np.append(data, appended_data)\n",
    "        splitted.append(data)\n",
    "        labels_multiplied = [label] * 1\n",
    "\n",
    "    # If it is the exact length, then don't alter it\n",
    "    elif ratio == 1:\n",
    "        splitted.append(data)\n",
    "        labels_multiplied = [label] * 1\n",
    "    return splitted, labels_multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9956d375-d243-44f7-b050-196890bf9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = []\n",
    "normalized_label = []\n",
    "time_set = 20\n",
    "duration = time_set*fs\n",
    "for j, data in enumerate(ecg_leads_edited):\n",
    "    ecg_split_data, lab_mul = ecg_split(data, duration, ecg_labels[j])\n",
    "    normalized.extend(ecg_split_data)\n",
    "    normalized_label.extend(lab_mul)\n",
    "\n",
    "ecg_leads_edited = normalized\n",
    "ecg_labels_edited = normalized_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688742a9-1139-495b-8944-1ac174cba3fc",
   "metadata": {},
   "source": [
    "### Split training, validation, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5576ce55-cb5b-4a52-ab02-40ffd31b7b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(ecg_leads_edited, ecg_labels_edited, test_size=0.2, shuffle = True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c7ba090-4708-4bd1-aa77-0c6ef4ce7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef25a45-3bd5-4553-923f-223da86cf61f",
   "metadata": {},
   "source": [
    "### Random oversampling for imbalanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a81336e8-3967-4927-b3d9-988fc7249dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0, sampling_strategy='minority')\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Converts sublists to arrays\n",
    "xfiltered = []\n",
    "yfiltered = []\n",
    "for j, data in enumerate(X_train):\n",
    "    xfiltered.append(np.asarray(X_resampled[j]))\n",
    "    yfiltered.append(np.asarray(y_resampled[j]))\n",
    "X_train = xfiltered\n",
    "y_train = yfiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15646a95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Label encoding (0: A, 1: N, 2: O, 3: ~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c279e0a5-dc5d-4f3c-94d6-452d77f5cb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get indices for each label\n",
    "# idx_N = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == 'N']\n",
    "# idx_A = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == 'A']\n",
    "# idx_tilde = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == '~']\n",
    "# idx_O = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "07212e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelencode(y_data):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_data)\n",
    "    encoded = le.transform(y_data)\n",
    "    encoded = keras.utils.to_categorical(encoded)\n",
    "    return encoded\n",
    "\n",
    "def tolist(array):\n",
    "    liste = []\n",
    "    for i, data in enumerate(array):\n",
    "        liste.append(data)\n",
    "    return liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a21210b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = []\n",
    "# for p, data in enumerate(ecg_labels_edited):\n",
    "#     d.append(\n",
    "#         {\n",
    "#             'ecg_data': ecg_names[p],\n",
    "#             'label': ecg_labels_enc[p]\n",
    "#         }\n",
    "#     )\n",
    "# features_names = pd.DataFrame(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75aee684-7ad1-40a1-b1d4-dbc957df054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical & one-hot encode\n",
    "y_train = labelencode(y_train)\n",
    "y_test = labelencode(y_test)\n",
    "\n",
    "# Convert to list\n",
    "y_train = tolist(y_train)\n",
    "y_test = tolist(y_test)\n",
    "\n",
    "# Convert to array... x=(samples, time), y=(samples, encodings)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72425f1-3a69-4e79-b0de-9bb77eee8f2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification (CNN)\n",
    "- Sources:\n",
    "    - https://stackoverflow.com/questions/55233377/keras-sequential-model-with-multiple-inputs\n",
    "    - https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "    - https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n",
    "    - https://www.tutorialspoint.com/keras/keras_convolution_neural_network.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81737b6-ca40-46c5-9ba6-535195b08483",
   "metadata": {},
   "source": [
    "### CNN-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3122c92e-2e46-4976-ac3f-79b9a7e515f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1d(filters, kernel_size, act='relu'):\n",
    "    layer = tf.keras.layers.Conv1D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=1,\n",
    "        activation=act,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        data_format=\"channels_last\",\n",
    "        padding=\"valid\",\n",
    "        # dilation_rate=1,\n",
    "        # groups=1,\n",
    "        # kernel_regularizer=None,\n",
    "        # bias_regularizer=None,\n",
    "        # activity_regularizer=None,\n",
    "        # kernel_constraint=None,\n",
    "        # bias_constraint=None\n",
    "        )\n",
    "    return layer\n",
    "\n",
    "def maxpool(stride_num, pool_num=2):\n",
    "    layer = tf.keras.layers.MaxPooling1D(\n",
    "        strides=stride_num,\n",
    "        pool_size=pool_num,\n",
    "        padding=\"valid\",\n",
    "        # data_format=\"channels_last\"\n",
    "        )\n",
    "    return layer\n",
    "\n",
    "def fully(uni, act='relu'):\n",
    "    layer = tf.keras.layers.Dense(\n",
    "        units=uni,\n",
    "        activation=act,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=keras.regularizers.L2(1e-3),\n",
    "        # bias_regularizer=None,\n",
    "        # activity_regularizer=None,\n",
    "        # kernel_constraint=None,\n",
    "        # bias_constraint=None\n",
    "        )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d00f80-1da5-4a47-8f79-1b784e01fc02",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CNN (fan_multiscaled_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a3851b00-27c2-44a6-a4ed-796294dbe639",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s = 128\n",
    "epochen = 3\n",
    "num_classes = 4\n",
    "X_tr = X_train\n",
    "y_tr = y_train\n",
    "X_te = X_test\n",
    "y_te = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7425a22b-2a08-4e80-b1a1-0bf09dccc8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream1 = keras.layers.Input(\n",
    "    shape=(duration,1),\n",
    "    #batch_size=batch_s\n",
    ")\n",
    "stream2 = keras.layers.Input(\n",
    "    shape=(duration,1),\n",
    "    #batch_size=batch_s\n",
    ")\n",
    "\n",
    "# First branch\n",
    "line1 = conv_1d(64,3,act=None)(stream1)\n",
    "line1 = conv_1d(64,3)(line1)\n",
    "line1 = maxpool(3)(line1)\n",
    "line1 = conv_1d(128,3)(line1)\n",
    "line1 = conv_1d(128,3)(line1)\n",
    "line1 = maxpool(3)(line1)\n",
    "line1 = conv_1d(256,3)(line1)\n",
    "line1 = conv_1d(256,3)(line1)\n",
    "line1 = conv_1d(256,3)(line1)\n",
    "line1 = maxpool(2)(line1)\n",
    "line1 = conv_1d(512,3)(line1)\n",
    "line1 = conv_1d(512,3)(line1)\n",
    "line1 = conv_1d(512,3)(line1)\n",
    "line1 = maxpool(2)(line1)\n",
    "line1 = conv_1d(512,3)(line1)\n",
    "line1 = conv_1d(512,3)(line1)\n",
    "line1 = conv_1d(512,3)(line1)\n",
    "line1_out = maxpool(2)(line1)\n",
    "line1_mod = keras.Model(inputs=stream1, outputs=line1_out)\n",
    "\n",
    "# Second branch\n",
    "line2 = conv_1d(64,7,act=None)(stream2)\n",
    "line2 = conv_1d(64,7)(line2)\n",
    "line2 = maxpool(3)(line2)\n",
    "line2 = conv_1d(128,7)(line2)\n",
    "line2 = conv_1d(128,7)(line2)\n",
    "line2 = maxpool(3)(line2)\n",
    "line2 = conv_1d(256,3)(line2)\n",
    "line2 = conv_1d(256,3)(line2)\n",
    "line2 = conv_1d(256,3)(line2)\n",
    "line2 = maxpool(2)(line2)\n",
    "line2 = conv_1d(512,3)(line2)\n",
    "line2 = conv_1d(512,3)(line2)\n",
    "line2 = conv_1d(512,3)(line2)\n",
    "line2 = maxpool(2)(line2)\n",
    "line2 = conv_1d(512,3)(line2)\n",
    "line2 = conv_1d(512,3)(line2)\n",
    "line2 = conv_1d(512,3)(line2)\n",
    "line2_out = maxpool(2)(line2)\n",
    "line2_mod = keras.Model(inputs=stream2, outputs=line2_out)\n",
    "\n",
    "# Combination\n",
    "combined = keras.layers.Concatenate()([line1_mod.output, line2_mod.output])\n",
    "combined = keras.layers.Flatten()(combined)\n",
    "\n",
    "# Fully connected layer\n",
    "line3 = fully(1024, act=None)(combined)\n",
    "line3 = fully(1024)(line3)\n",
    "line3 = fully(256)(line3)\n",
    "outputs = fully(num_classes, act=\"softmax\")(line3)\n",
    "model = keras.Model(inputs=[stream1, stream2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ea383ef2-cb34-4e1f-b7f3-3a81bcab3cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_85\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_68 (InputLayer)          [(None, 6000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " input_69 (InputLayer)          [(None, 6000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_750 (Conv1D)            (None, 5998, 64)     256         ['input_68[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_763 (Conv1D)            (None, 5994, 64)     512         ['input_69[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_751 (Conv1D)            (None, 5996, 64)     12352       ['conv1d_750[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_764 (Conv1D)            (None, 5988, 64)     28736       ['conv1d_763[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_287 (MaxPooling1  (None, 1999, 64)    0           ['conv1d_751[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_292 (MaxPooling1  (None, 1996, 64)    0           ['conv1d_764[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_752 (Conv1D)            (None, 1997, 128)    24704       ['max_pooling1d_287[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_765 (Conv1D)            (None, 1990, 128)    57472       ['max_pooling1d_292[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_753 (Conv1D)            (None, 1995, 128)    49280       ['conv1d_752[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_766 (Conv1D)            (None, 1984, 128)    114816      ['conv1d_765[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_288 (MaxPooling1  (None, 665, 128)    0           ['conv1d_753[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_293 (MaxPooling1  (None, 661, 128)    0           ['conv1d_766[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_754 (Conv1D)            (None, 663, 256)     98560       ['max_pooling1d_288[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_767 (Conv1D)            (None, 659, 256)     98560       ['max_pooling1d_293[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_755 (Conv1D)            (None, 661, 256)     196864      ['conv1d_754[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_768 (Conv1D)            (None, 657, 256)     196864      ['conv1d_767[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_756 (Conv1D)            (None, 659, 256)     196864      ['conv1d_755[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_769 (Conv1D)            (None, 655, 256)     196864      ['conv1d_768[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_289 (MaxPooling1  (None, 329, 256)    0           ['conv1d_756[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_294 (MaxPooling1  (None, 327, 256)    0           ['conv1d_769[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_757 (Conv1D)            (None, 327, 512)     393728      ['max_pooling1d_289[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_770 (Conv1D)            (None, 325, 512)     393728      ['max_pooling1d_294[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_758 (Conv1D)            (None, 325, 512)     786944      ['conv1d_757[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_771 (Conv1D)            (None, 323, 512)     786944      ['conv1d_770[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_759 (Conv1D)            (None, 323, 512)     786944      ['conv1d_758[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_772 (Conv1D)            (None, 321, 512)     786944      ['conv1d_771[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_290 (MaxPooling1  (None, 161, 512)    0           ['conv1d_759[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_295 (MaxPooling1  (None, 160, 512)    0           ['conv1d_772[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_760 (Conv1D)            (None, 159, 512)     786944      ['max_pooling1d_290[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_773 (Conv1D)            (None, 158, 512)     786944      ['max_pooling1d_295[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_761 (Conv1D)            (None, 157, 512)     786944      ['conv1d_760[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_774 (Conv1D)            (None, 156, 512)     786944      ['conv1d_773[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_762 (Conv1D)            (None, 155, 512)     786944      ['conv1d_761[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_775 (Conv1D)            (None, 154, 512)     786944      ['conv1d_774[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_291 (MaxPooling1  (None, 77, 512)     0           ['conv1d_762[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_296 (MaxPooling1  (None, 77, 512)     0           ['conv1d_775[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 77, 1024)     0           ['max_pooling1d_291[0][0]',      \n",
      "                                                                  'max_pooling1d_296[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 78848)        0           ['concatenate_26[0][0]']         \n",
      "                                                                                                  \n",
      " dense_124 (Dense)              (None, 1024)         80741376    ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 1024)         1049600     ['dense_124[0][0]']              \n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 256)          262400      ['dense_125[0][0]']              \n",
      "                                                                                                  \n",
      " dense_127 (Dense)              (None, 4)            1028        ['dense_126[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 91,984,004\n",
      "Trainable params: 91,984,004\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c550ae-dc79-4c31-8250-26093cacb6de",
   "metadata": {},
   "source": [
    "### Compile & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b554ca37-e319-4804-91de-c45ca3f67cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=300,\n",
    "    decay_rate=0.1)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    "    # loss_weights=None,\n",
    "    # weighted_metrics=None,\n",
    "    # run_eagerly=None,\n",
    "    # steps_per_execution=None,\n",
    "    # jit_compile=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "32d5c68d-ed86-48e4-8260-949ed5b60f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 199, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_85\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 6000) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [198]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m      2\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m      4\u001b[0m         filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_julian_\u001b[39m\u001b[38;5;132;01m{epoch}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m ]\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#batch_size = batch_s, \u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_te\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_te\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# shuffle=True,\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# class_weight=None,\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# sample_weight=None,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# initial_epoch=0,\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# steps_per_epoch=None,\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_steps=None,\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_batch_size=None,\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_split=0.0,\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_freq=1,\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_queue_size=10,\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# workers=1,\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use_multiprocessing=False,\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/tljh/user/envs/tensorflow/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 199, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_85\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 6000) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='cnn_julian_{epoch}',\n",
    "        save_freq='epoch',\n",
    "        monitor='val_loss',\n",
    "        mode='auto',\n",
    "        verbose=0,\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=2,\n",
    "        mode=\"auto\",\n",
    "        verbose=1,\n",
    "        # min_delta=0,\n",
    "        # baseline=None,\n",
    "        # restore_best_weights=False,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x = X_tr,\n",
    "    y = y_tr,\n",
    "    #batch_size = batch_s, \n",
    "    epochs = epochen, \n",
    "    verbose = 1,\n",
    "    validation_data = (X_te, y_te),\n",
    "    callbacks = callbacks,\n",
    "    # shuffle=True,\n",
    "    # class_weight=None,\n",
    "    # sample_weight=None,\n",
    "    # initial_epoch=0,\n",
    "    # steps_per_epoch=None,\n",
    "    # validation_steps=None,\n",
    "    # validation_batch_size=None,\n",
    "    # validation_split=0.0,\n",
    "    # validation_freq=1,\n",
    "    # max_queue_size=10,\n",
    "    # workers=1,\n",
    "    # use_multiprocessing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4812fb-3521-4e4e-856b-9649b93c45e4",
   "metadata": {},
   "source": [
    "### Test & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cd96f-27e8-43b5-88f6-21ba9240985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(\n",
    "    x=X_te,\n",
    "    y=y_te,\n",
    "    verbose = 'auto',\n",
    "    # batch_size=None,\n",
    "    # sample_weight=None,\n",
    "    # steps=None,\n",
    "    # callbacks=None,\n",
    "    # max_queue_size=10,\n",
    "    # workers=1,\n",
    "    # use_multiprocessing=False,\n",
    "    # return_dict=False\n",
    ") \n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808a9ee-84fb-4526-81fd-c36b25cf6023",
   "metadata": {},
   "source": [
    "### Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54b2fd-a93b-4ac6-b993-3420a93609d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "Model.save(\n",
    "    filepath=\"cnn_julian.tf\",\n",
    "    include_optimizer=True,\n",
    "    save_format=\"tf\",\n",
    "    # overwrite=True,\n",
    "    # signatures=None,\n",
    "    # options=None,\n",
    "    # save_traces=True,\n",
    ")\n",
    "#loaded_model = tf.keras.models.load_model('/tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcba624-d254-4042-b6c4-7e3f62cca566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model statistics\n",
    "model.summary()\n",
    "print(history.history)\n",
    "#model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecce03fc-2d7a-4b50-aeb3-dfed59351322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Plot model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(\n\u001b[1;32m      3\u001b[0m     model,\n\u001b[1;32m      4\u001b[0m     to_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     rankdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     expand_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      8\u001b[0m     show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     show_layer_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Misc.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     show_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     show_layer_activations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot model\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    rankdir=\"TB\",\n",
    "    dpi=80,\n",
    "    show_shapes=True,\n",
    "    layer_range=None,\n",
    "    show_layer_names=False,\n",
    "    \n",
    "    # Misc.\n",
    "    # show_dtype=True,\n",
    "    # show_layer_activations=True,\n",
    "    # expand_nested=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a49c2a-5ccc-431e-9451-426749b19938",
   "metadata": {},
   "source": [
    "### Predict & run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4f51a-7e2f-41fb-bd3f-d417d52cbc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(\n",
    "    x = X_te,\n",
    "    batch_size=None,\n",
    "    verbose=\"auto\"\n",
    "    \n",
    "    # Misc.\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")\n",
    "pred = np.argmax(pred, axis = 1)[:5] \n",
    "label = np.argmax(y_test,axis = 1)[:5] \n",
    "\n",
    "print(pred)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8f1c3-fb31-4375-bf53-b306b9b8333a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75a3fc99",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neuronales Netz aufstellen\n",
    "- [ ] Testlauf mit niedriger Epoche: verbose=0 in fit, falls Fehlermeldung\n",
    "\n",
    "## Tuningprozess\n",
    "- https://keras.io/keras_tuner/\n",
    "- https://keras.io/api/keras_tuner/tuners/\n",
    "- https://keras.io/guides/keras_tuner/getting_started/\n",
    "- https://keras.io/api/keras_tuner/hyperparameters/\n",
    "- https://keras.io/getting_started/intro_to_keras_for_engineers/#finding-the-best-model-configuration-with-hyperparameter-tuning\n",
    "- 10 fold Crossvalidation\n",
    "- [08 - Data Science & Machine Learning](:/67056066e2104708bcc4c6d27239e051)\n",
    "- Auch ROC, Confusion Matrix und Learning Curves ausleiten... (TP, TN, ...):\n",
    "    - https://keras.io/api/metrics/classification_metrics/\n",
    "    - https://keras.io/api/metrics/\n",
    "    - https://keras.io/api/metrics/accuracy_metrics/\n",
    "    - https://keras.io/api/metrics/probabilistic_metrics/\n",
    "    - https://towardsdatascience.com/building-a-multi-output-convolutional-neural-network-with-keras-ed24c7bc1178\n",
    "- [ ] Auf Supercomputer auslegen (Anleitung s. Moodle)\n",
    "    - https://keras.io/getting_started/faq/#how-can-i-train-a-keras-model-on-multiple-gpus-on-a-single-machine\n",
    "    - https://keras.io/getting_started/faq/#how-can-i-train-a-keras-model-on-tpu\n",
    "    - https://keras.io/api/callbacks/backup_and_restore/\n",
    "    - https://keras.io/getting_started/intro_to_keras_for_engineers/#speeding-up-training-with-multiple-gpus\n",
    "    - https://keras.io/getting_started/intro_to_keras_for_engineers/#doing-preprocessing-synchronously-ondevice-vs-asynchronously-on-host-cpu\n",
    "- [ ] weitere Trainingsdaten durchgehen (s. andere)\n",
    "\n",
    "## Sonstige\n",
    "- [ ] Normalisieren:\n",
    "    - https://keras.io/api/layers/preprocessing_layers/numerical/normalization/\n",
    "    - https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
    "- [ ] Rebalance: https://keras.io/examples/structured_data/imbalanced_classification/\n",
    "- [ ] Schnitttechnik der Daten (nur R-Peaks cutten)\n",
    "- [ ] Alternative Architekturen (notfalls einfach auch nur Code durchgehen, um zu ergänzen):\n",
    "    - https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
    "    - https://keras.io/examples/timeseries/timeseries_classification_transformer/\n",
    "    - https://keras.io/api/data_loading/timeseries/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
