{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dfa08c",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01046257-5167-465b-b119-42d118db081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /home/jupyter-wki_team_1/.local/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/tljh/user/lib/python3.9/site-packages (from imbalanced-learn) (1.22.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/jupyter-wki_team_1/.local/lib/python3.9/site-packages (from imbalanced-learn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/tljh/user/lib/python3.9/site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from imbalanced-learn) (1.1.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: '/opt/tljh/user/lib/python3.9/site-packages/google_pasta-0.2.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/tljh/user/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.19.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/tljh/user/lib/python3.9/site-packages (from scikit-image) (9.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.9/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/tljh/user/lib/python3.9/site-packages (from scikit-image) (1.22.3)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2022.5.4-py3-none-any.whl (195 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.6/195.6 KB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting imageio>=2.4.1\n",
      "  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /opt/tljh/user/lib/python3.9/site-packages (from scikit-image) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/tljh/user/lib/python3.9/site-packages (from scikit-image) (1.8.0)\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/tljh/user/lib/python3.9/site-packages (from packaging>=20.0->scikit-image) (3.0.8)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: '/opt/tljh/user/lib/python3.9/site-packages/google_pasta-0.2.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tifffile, networkx, imageio, scikit-image\n",
      "\u001b[33m  WARNING: The scripts lsm2bin, tiff2fsspec, tiffcomment and tifffile are installed in '/home/jupyter-wki_team_1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts imageio_download_bin and imageio_remove_bin are installed in '/home/jupyter-wki_team_1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script skivi is installed in '/home/jupyter-wki_team_1/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed imageio-2.19.3 networkx-2.8.4 scikit-image-0.19.3 tifffile-2022.5.4\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/tljh/user/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install imbalanced-learn\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc002ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import load_references, save_predictions\n",
    "from preprocess import *\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37d0ff50-edab-4940-ae65-de168fef9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"training.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dbd17",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e211183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\t Dateien wurden geladen.\n"
     ]
    }
   ],
   "source": [
    "ecg_leads, ecg_labels, fs, ecg_names = load_references(folder='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d7172",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58542dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_leads_edited = ecg_leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0884f",
   "metadata": {},
   "source": [
    "### Noise removal (fan_multiscaled_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e3f82af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = []\n",
    "for j, data in enumerate(ecg_leads_edited):\n",
    "    data_ftt, freq = ecg_furier(ecg_leads_edited[j], fs)\n",
    "    lowpassed = ecg_denoise_spectrum(data_ftt, freq, 0, 60)\n",
    "    filtered.append(ecg_invfurier(lowpassed))\n",
    "\n",
    "ecg_leads_edited = filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5707d67",
   "metadata": {},
   "source": [
    "### Z-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf7ef2a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = []\n",
    "for j, data in enumerate(ecg_leads_edited):\n",
    "    filtered.append(ecg_norm(ecg_leads_edited[j]))\n",
    "\n",
    "ecg_leads_edited = filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c37446-9c40-4bb4-b388-a19e052fdf59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Length normalization (hsieh_detection_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e71ea78-7b38-42d8-8a3d-ea2b9ad833e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecg_split(data, limit, label):\n",
    "    splitted = []\n",
    "    ratio = len(data)/limit\n",
    "\n",
    "    # If data is longer than limit, split with at least 50% overlap\n",
    "    if ratio > 1:\n",
    "        for i in range(0, int(np.ceil(2*ratio))):\n",
    "            if i == int(np.ceil(2*ratio))-1:\n",
    "                string = data[len(data)-limit:len(data)]\n",
    "            else:\n",
    "                string = data[int(np.floor(i*limit/2)):int(np.floor(i*limit/2+limit))]\n",
    "            splitted.append(string)\n",
    "        splitted = [x for x in splitted if x.shape[0] >= limit] # remove string which are shorter\n",
    "        labels_multiplied = [label] * len(splitted)\n",
    "\n",
    "    # If data is shorter than limit, add from the beginning\n",
    "    elif ratio < 1:\n",
    "        if ratio <= 0.5:\n",
    "            data = np.tile(data, int(np.floor(1/ratio)))\n",
    "        diff = limit - len(data)\n",
    "        appended_data = data[0:diff]\n",
    "        data = np.append(data, appended_data)\n",
    "        splitted.append(data)\n",
    "        labels_multiplied = [label] * 1\n",
    "\n",
    "    # If it is the exact length, then don't alter it\n",
    "    elif ratio == 1:\n",
    "        splitted.append(data)\n",
    "        labels_multiplied = [label] * 1\n",
    "    return splitted, labels_multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9956d375-d243-44f7-b050-196890bf9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = []\n",
    "normalized_label = []\n",
    "duration = 20\n",
    "split = duration*fs\n",
    "for j, data in enumerate(ecg_leads_edited):\n",
    "    ecg_split_data, lab_mul = ecg_split(data, split, ecg_labels[j])\n",
    "    normalized.extend(ecg_split_data)\n",
    "    normalized_label.extend(lab_mul)\n",
    "\n",
    "ecg_leads_edited = normalized\n",
    "ecg_labels_edited = normalized_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688742a9-1139-495b-8944-1ac174cba3fc",
   "metadata": {},
   "source": [
    "### Split training, validation, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5576ce55-cb5b-4a52-ab02-40ffd31b7b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(ecg_leads_edited, ecg_labels_edited, test_size=0.2, shuffle = True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c7ba090-4708-4bd1-aa77-0c6ef4ce7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef25a45-3bd5-4553-923f-223da86cf61f",
   "metadata": {},
   "source": [
    "### Random oversampling for imbalanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a81336e8-3967-4927-b3d9-988fc7249dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0, sampling_strategy='minority')\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Converts sublists to arrays\n",
    "xfiltered = []\n",
    "yfiltered = []\n",
    "for j, data in enumerate(X_train):\n",
    "    xfiltered.append(np.asarray(X_resampled[j]))\n",
    "    yfiltered.append(np.asarray(y_resampled[j]))\n",
    "X_train = xfiltered\n",
    "y_train = yfiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15646a95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Label encoding (0: A, 1: N, 2: O, 3: ~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279e0a5-dc5d-4f3c-94d6-452d77f5cb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get indices for each label\n",
    "# idx_N = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == 'N']\n",
    "# idx_A = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == 'A']\n",
    "# idx_tilde = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == '~']\n",
    "# idx_O = [i for i in range(len(ecg_labels_edited)) if ecg_labels_edited[i] == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07212e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(ecg_labels_edited)\n",
    "ecg_labels_enc = le.transform(ecg_labels_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21210b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = []\n",
    "# for p, data in enumerate(ecg_labels_edited):\n",
    "#     d.append(\n",
    "#         {\n",
    "#             'ecg_data': ecg_names[p],\n",
    "#             'label': ecg_labels_enc[p]\n",
    "#         }\n",
    "#     )\n",
    "# features_names = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72425f1-3a69-4e79-b0de-9bb77eee8f2e",
   "metadata": {},
   "source": [
    "# Classification (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3fc99",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neuronales Netz aufstellen\n",
    "- [ ] nur auf Trainingsdaten\n",
    "- [ ] Tuningprozess: [08 - Data Science & Machine Learning](:/67056066e2104708bcc4c6d27239e051)\n",
    "- [ ] Tuning der Parameter: https://keras.io/guides/keras_tuner/getting_started/\n",
    "- [ ] Auch ROC, Confusion Matrix und Learning Curves ausleiten... (TP, TN, ...)\n",
    "- [ ] Schnitttechnik der Daten (nur R-Peaks cutten)\n",
    "- [ ] Auf Supercomputer auslegen (Anleitung s. Moodle)\n",
    "- [ ] weitere Trainingsdaten durchgehen (s. andere)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
